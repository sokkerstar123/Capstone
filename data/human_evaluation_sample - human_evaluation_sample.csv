assigned_to,title,published,url,video_id,channel_id,full_transcript_length,baseline_summary,rag_summary,human_summary,rag_flan_t5_lg,rag_flan_t5_after_gemini_2.5flash,open-router-gpt-oss-20b,quality_of_baseline,quality_of_rag,quality_of_human,quality_of_rag_flan_t5_lg,quality_rag_flan_t5_after_gemini_2.5flash,relevance_of_baseline,relevance_of_rag,relevance_of_human,relevance_of_rag_flan_t5_lg,relevance_of_rag_flan_t5_after_gemini_2.5flash,informative_of_baseline,informative_of_rag,informative_of_human,informative_of_rag_flan_t5_lg,informative_of_rag_flan_t5_after_gemini_2.5flash
Yingying,What Is Neuralink??! - TDBS 22 April 2017,2017-04-23 11:37:29,What Is Neuralink??! - TDBS 22 April 2017,ToZyCA0uvoI,UCr8O8l5cCX85Oem1d18EezQ,10811,Daniel Burke is standing up and doing this live show on YouTube. Usually I do it in my car. If you're watching on YouTube you can see all this. That's in the sun. Now that's way too hot.,"You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary. Today's workout focused on handstands.","A vlog video by Daniel Burke, overall its a discussion of what Neuralink is and how it works.",Daniel Burke is doing a live show on YouTube.,Elon Musk has started a company called Neuralink. He's putting together an elaborate team of some of the smartest people in the world. And what the company wants to do is find a way to link the human brain with a super powerful or super intelligent artificial intelligence.,"Day 81 of the Daniel Burke Show was filmed live on YouTube in a sunny standing‑up set,",1,1,4,3,4,1,1,5,1,5,1,1,4,1,4
Yingying,[ML News] Hugging Face course | GAN Theft Auto | AI Programming Puzzles | PyTorch 1.9 Released,2021-06-24 12:35:09,https://youtu.be/6_q9DbX35kk,6_q9DbX35kk,UCZHmQk67mSJgfCCTn7xBfew,13409,Huggingface releases a course you can now play GTA inside of an AI's mind and spot turns one. Facebook is testing AI to get you to stop fighting in its group CNN business sites.,Huggingface releases a course you can now play GTA inside of an AI's mind and spot turns one. Uber engineering blog has a new post up about how Uber switched from Xgas to to beat other at least single GPU models by quite a bit.,"A ML News video that discusses Hugginf Face, how Facebook uses AI to help moderators, Mcdonalds using AI in the drive through and how UBS CEO says AI won't replace humans.","Huggingface the famous NLP startup releases a course that teaches you how to use their models, libraries and other code they release.","I'm a big fan of this show. It's a lot of fun. I've got a few things to talk about, but I'd like to start off with a little bit about the show itself. First of all, it's an irregular show, so you have to be a data scientist or data scientist to watch it. Second, there are a couple of things that you can learn from the show. First, there is a GTA version that is played entirely in the mind of a neural network. Third, Facebook is testing AI to get you to stop fighting in its group CNN business sites.","Hugging Face has launched a free, hands‑on course that guides programmers through transformers, fine‑tuning, datasets, tokenizers, and advanced training techniques, making NLP accessible for developers and data scientists. Meanwhile, a new GTA‑style demo lets a neural network generate the entire game world from scratch, demonstrating physics learning without explicit modeling, though it requires a CUDA‑capable GPU. Facebook is testing AI‑driven moderation tools to alert admin moderators to toxic arguments in groups, sparking concerns about potential bias, while the British‑built autonomous vessel Mayflower 400 attempted a transatlantic crossing but had to return due to technical issues, highlighting the practical challenges of true end‑to‑end maritime autonomy.",3,2,5,4,3,2,2,5,3,2,2,2,5,2,2
Yingying,I TRAINED AN AI TO SOLVE 2+2 (w/ Live Coding),2020-08-06 16:30:14,https://youtu.be/udS2OPohs_s,udS2OPohs_s,UCZHmQk67mSJgfCCTn7xBfew,9454,Today we're going to build an AI that's going to answer the question what is 2 plus 2. We'll train nine thousand times 64 images and the air is gonna learn what two plus two is alright. We need a labeled data,"You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary.",An implementation  of someone uses Generative Adervarial Network to train an AI to solve 2+2.,The CUDA-based AI model we're going to build is a generative adversarial network.,We're going to build an AI that's going to answer the question what is 2 plus 2 now first thing we need is the model itself now the model is composed of a generator and a discriminator the generator is right here blink-blonk let's plop that in here that looks good,"The speaker mockingly critiques the “2 + 2 = 4 or 5” debate, declaring that AI must replace humans to answer the question. He outlines a ludicrous plan to build a GAN in PyTorch, using the MNIST digit dataset to “train” the model on synthetic images of the expression “2 + 2,” even fabricating the plus sign and labeling training pairs with simple arithmetic. The result is a comedic, technically confused script demonstrating how not to design a machine‑learning solution.",2,1,4,3,2,4,1,5,3,4,2,1,3,3,3
Yingying,Code with me (live): How to make your first Kaggle submission from scratch!,2018-12-02 5:03:43,https://youtu.be/f1y9wDDxWnA,f1y9wDDxWnA,UCr8O8l5cCX85Oem1d18EezQ,104808,Kashuk is a machine learning engineer at the University of Michigan. He uses Kaggle to explore a data set from scratch. Kashuk uses pandas to learn more about data analysis.,"You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary.","A livestream video where the creator walks through an exploratory data analysis (EDA) project using the Titanic dataset from Kaggle. The session covers downloading the dataset, exploring it in a Jupyter notebook, and making a Kaggle submission. The host also explains what EDA is, how to approach a dataset from scratch, and gives step-by-step reasoning for beginners learning pandas and machine learning basics, with shoutouts to Coursera’s Applied Data Science with Python course that inspired the tutorial.",The Daniel Burke Show is live on YouTube.,Kaggle: Titanic Machine Learning from Disaster - Part 1 - Duration: 2:29.08 - 1:28.,"The presenter launches his first live stream—facing connection hiccups but keeping the chat open—while outlining a step‑by‑step tutorial on how to perform exploratory data analysis on Kaggle’s Titanic dataset and build a winning submission. He explains the purpose of EDA for uncovering patterns before modeling, notes that the code will be released on GitHub, and thanks Coursera for sponsorship and a free trial offer. The session is interactive, with viewers encouraged to ask questions and provide feedback as he demonstrates the process in real time.",4,1,5,3,1,3,1,5,1,1,4,1,3,1,1
Yingying,"Loading in your own data - Deep Learning basics with Python, TensorFlow and Keras p.2",2018-08-18 13:50:58,https://youtu.be/j-3vuBynnOE,j-3vuBynnOE,UCfzlCWGWYyIQ0aLC5w48gBQ,16690,"Part two of our deep learning with Python, TensorFlow, and Keras tutorial. In this tutorial what we're going to be talking about is how to load in an outside data set. The data set we're using is this","This tutorial is part three of our deep learning with Python, TensorFlow, and Keras tutorial. In this tutorial we're going to be talking about how to load in an outside data set. In the next tutorial we'll actually train the neural network.","A tutorial video on Deep Learning with Python, TensorFlow, and Keras focusing on loading and preprocessing an external dataset, specifically the Cats vs. Dogs dataset from Microsoft/Kaggle","In this tutorial, we're going to be talking about how to load in an outside data set.",We're going to use matplotlib to show the image we're gonna use OS to iterate through directories and join paths cv2 to show images of dog and cat,"In this second part of the tutorial, the instructor shows how to download and explore the Kaggle cats‑and‑dogs dataset, importing NumPy, Matplotlib, OpenCV, and OS to iterate through the images. Using OpenCV, each image is read, converted to grayscale to reduce data size, displayed with Matplotlib, and then resized to a uniform 50×50 pixel format to simplify training a neural network. The walkthrough highlights common preprocessing steps—grayscale conversion, resizing, and dataset inspection—before moving on to model training.",4,4,4,3,2,3,3,4,2,3,3,3,5,2,3
Yingying,5 Handstand Progression Exercises You Should Be Doing,2017-11-21 10:57:33,https://youtu.be/wlLydjaHSWc,wlLydjaHSWc,UCr8O8l5cCX85Oem1d18EezQ,8173,"Bilzy uses a roller to open up the thoracic so he can lock out when he's holding his body overhead. He also uses a barbell to pancake his calf, removing all the soft tissue.","You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary. Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.","A fitness training video focused on handstand progression exercises. The hosts demonstrate mobility and strength drills to improve shoulder, thoracic spine, and wrist flexibility, including foam rolling, triceps and lat stretches with a band, and wrist prep routines.","Then, he explains how to activate your glutes.","Open up the shoulders, open up the thoracic. Open up your triceps. Open your lats. Take a step back with your left foot. Make sure your wrists are working.","The video opens with Bilzy explaining a calf “pancaking” technique he learned online to improve ankle mobility, before cutting to a handstand workshop. The instructor leads the group through a series of mobility and warm‑up drills—thoracic spine rolls, triceps and lat stretches, wrist rotations and finger‑push‑ups—followed by wall handstand progressions that emphasize staying hollow, keeping the back straight, and building stability. The session balances mobility work with concrete body‑weight drills to prepare participants for upside‑down practice.",3,1,4,1,2,4,1,4,1,2,4,1,5,1,2
Yingying,Data leakage during data preparation? | Using AntiPatterns to avoid MLOps Mistakes,2021-08-10 10:00:08,https://youtu.be/8_UBDTHAHqY,8_UBDTHAHqY,UCobqgqE4i5Kf7wrxRxhToQA,6541,"Machine learning is not just about putting machine learning models into production, but also about maintaining the whole operation. The paper guides us through anti-patterns ranging from technical errors to cases of missing knowledge about the context in which machine learning is",This week's episode features an expert summarizer. Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Tech.,"An AI Coffee Break with Letitia video that explains the concept of data leakage in MLOps, based on the paper “Using AntiPatterns to Avoid MLOps Mistakes.”",Ms. Coffee Bean read a paper about machine learning and machine learning methods.,"I-pattern to be the most interesting one. But the write-up was quite dense, since the paper is made for experts. Well, let's make it accessible to anyone by explaining this together. Alright, Ms. Coffee Bean, this paper is sharing lessons about MLOps, which is all about putting machine learning models into production, but also about maintaining the whole operation. However, the lessons of this paper are not about what to do and good strategies and patterns, but more about what not to do, so about the antipatterns.","The speaker recruits Ms. Coffee Bean to unpack a paper on MLOps antipatterns, arguing that the real value lies in understanding what *not* to do when putting models into production.  Focusing on the data leakage antipatterns, they detail four sneaky ways peekaboo, temporal leakage, oversampling, and œmetrics from beyond that can inflate performance by contaminating the train test split, emphasizing that proper data separation, domain knowledge of every feature, and a strict order of preprocessing steps are essential to avoid overoptimistic results.  Though drawn from finance, the guidelines are broadly applicable across any machine learning deployment.",3,4,4,3,3,3,3,4,2,4,3,4,4,1,3
Yingying,Machine Learning Holidays Live Stream,2021-12-27 14:18:58,https://youtu.be/-0ce136MdsA,-0ce136MdsA,UCZHmQk67mSJgfCCTn7xBfew,29789,"Dwi'n rhaid i mi gael sian ar y cyfnod, os na fyddai ddim yn gallu ychwanegu chi. I un adeg, rydyn ni'","You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary. The Hebbian non-Hebbian distinction was, do we learn the things like GPT three, but with a special focus on French.","A livestream video by Yannic Kilcher continuing his previous session on training a question-generation model using Hugging Face and Weights & Biases. The stream focuses less on model theory and more on the “plumbing” ,saving, loading, and managing models and datasets","Then, we'll freeze the learning rules.","Am ddysgu'r model generig. Rydym yn ymwneud Ã¢'r rhain. Roeddech chi'n cyfrif ar y model, oherwyddi hyperparameter, ond 1000 data points.","The speaker discusses training a language model (likely a pre‑trained T5) on a small dataset of about 1,000 data points, experimenting with hyper‑parameters and training for a few epochs while splitting the",2,1,4,1,1,1,1,3,1,1,1,1,3,1,1
Yingying,Author Interview - Transformer Memory as a Differentiable Search Index,2022-04-17 22:14:08,https://youtu.be/C7mUYocWdG0,C7mUYocWdG0,UCZHmQk67mSJgfCCTn7xBfew,39004,This is an interview with the authors of the paper transformer memory as a differentiable search index. Yi Tei and Don Metzler talk about the research project itself and what went well and what didn't.,"This is an interview with the authors of the paper transformer memory as a differentiable search index. During this interview, you will learn much more about the paper itself but also the research project itself what went well what didn't and what the authors think of the future of the field. As always, if you like content like this, consider sharing it out, leave a like if you liked it and leave a comment to let me know what you think.","In this interview, Yannic Kilcher speaks with Yi Tay and Donald Metzler about their paper “Transformer Memory as a Differentiable Search Index.” They explain how transformers can directly store and retrieve documents from their parameters, replacing traditional search indexes with a fully differentiable system.",This is an interview with the authors of the paper transformer memory as a differentiable search index.,"I'm here with Yi Tei and Don Metzler, who are authors of the paper transformer memory as a differentiable search index which I find really cool, really inspiring and very happy that you are here.","The interview discusses the authors’ paper on “Transformer Memory as a Differentiable Search Index,” highlighting how they explored storing entire datasets within transformer weights to test the model’s ability to memorize and retrieve documents. They explain the idea emerged from examining transformers’ memorization capabilities and from a broader vision of applying modern NLP to information retrieval, aiming to build a model that understands relationships between tokens and entire documents. The conversation also touches on the research process, motivations, and future directions for using transformer-based indices in search tasks.",4,4,4,3,3,4,4,4,4,4,4,4,4,3,3
Yingying,Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained),2020-07-04 12:39:13,https://youtu.be/hAooAOFRsYc,hAooAOFRsYc,UCZHmQk67mSJgfCCTn7xBfew,38086,"Transformers or RNNs, fast autoregressive transformers with linear attention by Angelos Kateropoulos, Apor Vias, Nikolaos Papas and Francois Fleuret. They also show that in the case of aut","This is an interview with the authors of the paper transformer memory as a different search index. The good thing about a transformer is that in such a sequence, it you can basically mainly look at the video attention is all you need. You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary.","A video on Fast Autoregressive Transformers with Linear Attention that explains how the authors reinterpret transformer attention as a kernel-based linear operation, reducing computational cost from quadratic to linear.","This video is about transformers or RNNs, fast autoregressive transformers with linear attention by Angelos Kateropoulos, Apor Vias, Nikolaos Papas and Francois Fleuret.","Ent to a special kind of RNN. So yeah, that's what this paper is about.","The speaker explains a recent paper on fast autoregressive transformers that use linear attention by interpreting the attention mechanism as a kernel, making the transformer an order‑of‑magnitude faster linear transformer and essentially equivalent to an RNN. They briefly recap how transformers work—generating key, query, and value vectors for each token, computing a softmaxed inner product to aggregate information from all tokens across layers. The video also touches on the broader context of transformer models (BERT, GPT) and the benefits of community feedback.",3,4,4,4,1,4,3,3,4,1,4,3,3,3,1
Damian,AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning,2019-11-02 11:23:04,https://youtu.be/BTLCdge7uSQ,BTLCdge7uSQ,UCZHmQk67mSJgfCCTn7xBfew,28671,DeepMind's Oriol Vinyals et al have published a paper in the Journal of Nature. The article describes how to train a reinforcement learning agent to play the game of Starcraft 2. Vinyal: This is a publicity stunt,DeepMind's AlphaStar Grandmaster level in Starcraft 2 using multi agent reinforcement learning. The corresponding paper looks like this and is by Oriol Vinyals et al from DeepMind and has been published in the Journal of Nature.,"A video about AlphaStar: an implementation of reinforcement learning being used on the game StarCraft II. With the goal of assessing if an AI model using reinforcement learning can reach the top league in esports, without any restrictions or handicaps given to the model",The paper that kicked off deep learning is a publicity stunt by DeepMind.,"Starcraft 2 is a real time strategy game. There are three races you can play, Terran, Protoss, and Zerg. The game lasts about 10 to 15 minutes. And the goal is to destroy the enemy base.","The speaker critiques DeepMind’s decision to publish its AlphaStar paper in the non‑open‑access journal Nature while arguing the research should be freely available, and then overviews the game of StarCraft II—its RTS mechanics, balanced races, and high‑action rate. They explain the extreme challenge of training a reinforcement‑learning agent to play over 10‑minute matches with a vast action space, and outline DeepMind’s approach, which starts with supervised imitation of high‑elo human games, extracts strategic build‑order statistics, and later employs a novel “leak training” technique to combine many RL tricks into a Grandmaster‑level bot.",2,3,5,2,3,2,4,4,2,3,3,3,5,1,2
Damian,[ML News] Cedille French Language Model | YOU Search Engine | AI Finds Profitable MEME TOKENS,2021-11-18 14:54:52,https://youtu.be/FC-R4MlIqrc,FC-R4MlIqrc,UCZHmQk67mSJgfCCTn7xBfew,34767,Weights and Biases is your one stop shop for all your machine learning needs. It will track your experiments with a single line of code. All of this is done automatically in a distributed way. You can literally sit on your toilet,Weights and Biases is your one stop shop for all your machine learning needs. It will track your experiments with a single line of code. All of this is done automatically in a distributed way. You can sit on your toilet on your smartphone and tune your hyper parameters and start new experiments.,"A news video that talks about upcoming news in AI and ML, topics discussed are DeepMinds Opensource model, a company being sued and the state of AI report for 2021",This video is sponsored by Weights and Biases. Weights and Biases is your one stop shop for all your machine learning needs. It will track your experiments with a single line of code. It will upload automatically all your,"We're going to talk about the first ever multilingual model to win WMT, a competition essentially to do machine translation. This is a corpus of data sets, but then also every year the competition hosts human expert translators that rate the translations of the machine t.","The video begins with an upbeat note about Weights & Biases, a cloud based tool that tracks machine learning experiments, logs, hyper parameters, data sets, and model artifacts, offering free personal and academic plans and self hosting options. It then reports two machine learning breakthroughs: a 6 B French GPT J style model called *City* that outperforms GPT 3 on French NLP tasks and is open source, and a Facebook AI multilingual model that has, for the first time, outperformed all bilingual models in the annual WMT translation competition by leveraging multi language training and advanced techniques. The segment concludes by announcing a new private search engine named U that promises privacy preserving web summaries, positioning it as a direct competitor to Google.",1,1,5,2,4,1,1,5,2,5,2,2,5,2,4
Damian,Starting Term 2 of the Udacity Artificial Intelligence Nanodegree | Learning Intelligence 19,2018-02-04 18:30:01,https://youtu.be/gxMOoNQEIjE,gxMOoNQEIjE,UCr8O8l5cCX85Oem1d18EezQ,12539,This week's episode is about the Artificial Intelligence Nanodegree on Udacity. This term is all about deep learning. The first project is computer vision capstone. Building a facial key point detection system.,"This week's episode of Learning Intelligence 19 is all about deep learning. The first project is computer vision capstone, building a facial key point detection system. In the next tutorial we'll actually train the neural network.",apart of a video series by Daniel Bourke on on his AI series. In this video he implemented a facial recognician model implementing it in Jupyter Notebook. ,This week's episode is going to be really cool. I'm into the second term of the Artificial Intelligence Nanodegree on Udacity. And this term is all about deep learning. So we're building projects using,"We've just completed part one of the computer vision Capstone project, and I'm going to get on to part two and part three later this week. I want to make a blog post or a video about it.","The speaker is halfway through the second term of Udacity’s AI Nanodegree, focusing on deep learning and a computer‑vision capstone that builds a facial key‑point detection system. They describe setting up an Anaconda environment, using $100 in AWS credits, and learning the basics of computer vision—perception and action—while showcasing an Affectiva demo that identifies emotions in real time. The speaker is documenting each step of the capstone, from face detection and edge enhancement to blurring and denoising, with plans to finish training a CNN for key‑point detection and later publish a blog or video guide.",1,1,4,2,3,2,2,4,2,2,1,1,4,1,2
Damian,Intro to APIs in Python - API Series #1,2021-09-29 14:00:18 UTC,https://youtu.be/g_yMowQikOE,g_yMowQikOE,UCv83tO5cePwHMt1952IVVHw,20115,"A look at a few different Python frameworks that we can use for putting them together. API, to start with, stands for Application Program Interface. In reality, this API is a script on a server somewhere.","Part two of our deep learning with Python, TensorFlow, and Keras tutorial. In this tutorial what we're going to be talking about is how to load in an outside data set. We'll use a cats and dogs data set from Microsoft.","An instruction video on a different methods to implement APIs in python, discussed how APIs work and went over basic HTTP and how the different HTTP Methods work, essential for learning on APIs work.","We're going to put together a series of videos just covering APIs, what APIs are, how we interact with them, how we build them.","RESTful APIs are a very common way of working. So we need to follow these six key factors in order to make sure our API is Restful or a REST API. So the first of those is we use a single outward-facing interface. There should be client-server independence. So like we saw before, over here on the left, that is the internet. And that is where we are coming from. Over here is a server.","The video introduces APIs as black‑box “middlemen” that let clients retrieve, create, update, or delete data on a server, with most interactions following the RESTful model governed by six principles—single entry point, client‑server independence, statelessness, caching, a layered architecture, and optionally code exposure. Common HTTP verbs are explained: GET for retrieval, POST for creation, PUT for replacement, DELETE for removal, and PATCH for partial",1,2,5,4,3,4,1,4,4,4,4,1,4,1,3
Damian,[ML News] Uber: Deep Learning for ETA | MuZero Video Compression  | Block-NeRF | EfficientNet-X,2022-02-18 16:38:08,https://youtu.be/fEKZC9mta8w,fEKZC9mta8w,UCZHmQk67mSJgfCCTn7xBfew,24761,Weights and Biases integration with the OpenAI API. Uber now uses deep learning to predict arrival times. mu zero is used to compress YouTube videos and nerf scales to entire city blocks.,Weights and Biases is your one stop shop for all your machine learning needs. It will track your experiments with a single line of code. All of this runs in the cloud. There are options to self host the system.,"ML News video, talks about how Uber uses DeepLearning to improve thier predictions of arrival times, discussed Tenrors and a tool called Know your data.","ML News: Uber now uses deep learning to predict arrival times, mu zero is used to compress YouTube videos and nerf scales to entire city blocks. Amazing.","Uber is a massive business. It's not only ride sharing its packages, its food, and all of these things have in common that at some point, there needs to be made a prediction of how long something is going to take until it arrives, either the food, the people, the packages, you name it. So they switched from Xg boost to deep learning to predict arrival times. Now what's pretty interesting right here is that the goal isn't necessarily to predict the arrival time. However, they have a traffic routing system already, which is essentially something like Google Maps, you type in where you want to go and where you are.","Weights & Biases now integrates with the OpenAI API, letting users fine‑tune GPT‑3 models on their own data and automatically sync training runs, artifacts, tables, and analysis—all viewable within W&B’s interactive dashboards. Uber, moving away from an XGBoost‑based ETA model, adopted a deep‑learning transformer that predicts residuals to a traffic routing system, carefully handling spatial features and asymmetric loss to improve arrival‑time accuracy in milliseconds. DeepMind announced the first real‑world deployment of MuZero, a model‑free reinforcement‑learning algorithm that learns policies without a pre‑defined environment model.",3,2,4,2,4,4,1,5,3,5,2,2,4,3,4
Damian,Talking to companies at ICML19,2019-06-13 7:13:32,https://youtu.be/hkw-WDBipgo,hkw-WDBipgo,UCZHmQk67mSJgfCCTn7xBfew,5091,Graphcore is the new kid on the block for AI hardware. They claim they have a chip specifically designed for the types of operations that machine learning applications do. For now they just offer a cloud solution.,"Graphcore is a new kid on the block for AI hardware, in that they claim they have a chip specifically designed for the types of operations that machine learning applications do. For now they just offer a cloud solution, I believe, and they're going to sell their cards through Dell.","A vlog video where Yannic went to a couple of different locations and talked about different implementations of ML and AI in 2019, additionally he talked abouta wellness center given out free massages.",Is it possible to combine GPUs and CPUs?,"I'm a big fan of the NSA. I've talked to them about machine learning, and they're like, yeah, there are problems like this. And you ask them what do you do?","At conferences he notes that companies vary between showing off cutting‑edge tech—like Graphcore’s AI chips and a mini‑LiDAR sensor—and simply recruiting talent, which can make conversations feel vague or secretive. He candidly shares that even speaking to a high‑profile entity like the NSA yields broad, classified answers, but he still finds value in probing companies to uncover real research needs. Although he feels he isn’t the best at networking, he hopes others can offer strategies to make these industry interactions more fruitful.",2,2,5,2,2,1,1,4,2,1,1,1,4,1,1
Damian,Agent57: Outperforming the Atari Human Benchmark,2020-03-31 15:49:49,https://youtu.be/U3zmekzQ8WQ,U3zmekzQ8WQ,UCZHmQk67mSJgfCCTn7xBfew,21793,Agent 57 is a new agent by DeepMind that is the first one to beat all of the 57 games in the Atari suite to a human or superhuman performance. Agent 57 builds on a number of previous improvements to the original DeepQ networks,"You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary. Today we'll look at Agent 57, a new agent by DeepMind.","A video about Agent57, which is the first model to be able to beat all of the games in the atari series, after the discussion on what Agent57 is the user went over how Reinforcement works in the backend and how it was implemented in Agent 57",The new DeepMind agent beats all of the 57 games in the Atari suite to a human or superhuman performance.,"Let's go into what deep Q learning is. So in reinforcement learning, we have a neural network taking in the observation. So this is O, the observation goes in here. And the action goes in there. Now let's call this AI because we have different actions. So you put observation with action AJ in the same network, right?","Agent 57 is DeepMind’s first reinforcement‑learning agent to beat all 57 games in the Atari benchmark, including hard ones like Solaris where rewards are sparse and long‑delayed. The talk explains how classic Deep Q‑Networks (DQN) model the game state with a neural network to predict Q‑values, but struggle with credit‑assignment over long horizons, and how newer methods such as R2D2 add recurrent networks to encode past observations and actions, improving performance on these challenging tasks.",4,1,4,4,2,4,1,4,3,3,5,1,5,2,2
Damian,Deep Ensembles: A Loss Landscape Perspective (Paper Explained),2020-07-11 12:57:17,https://youtu.be/5IRlUVrEVL8,5IRlUVrEVL8,UCZHmQk67mSJgfCCTn7xBfew,38651,"This week we look at the loss landscape of deep ensembles, a loss landscape perspective by Stanislav Fort, Hui Hu and Balaji Lakshminarayanan. I generally enjoy this type of research because it's kind","Today we'll look at deep ensembles, a loss landscape perspective by Stanislav Fort, Hui Hu and Balaji Lakshminarayanan. This was the paper right here, the paper that kicked off deep learning. So we'll go through the paper, what it does, what evolutionary methods really are. And then we'll then look at the experiments that's going to be used in the paper.",A video about how Deep Ensembles work and went over a book called A Loss Landscape Perspective and summarized the book.,"Today we'll look at deep ensembles, a loss landscape perspective by Stanislav Fort, Hui Hu and Balaji Lakshminarayanan.","Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out of distribution robustness of deep learning models. So what are deep ensembles really quick and an ensemble model?","The transcript explains the recent paper “Loss Landscape Perspective on Deep Ensembles” by Fort, Hu, and Lakshminarayanan, which shows through experiments that each randomly initialized member of a deep ensemble converges to a distinct local minimum in weight space, thereby covering multiple modes of the functional solution space. This diversity explains why deep ensembles often outperform Bayesian neural networks, which usually capture only a single mode, and why they improve accuracy, uncertainty, and out‑of‑distribution robustness. The speaker also discusses how ensembles are built, why they help generalization, and how this explanatory research remains accessible to academia despite the growing trend toward resource‑heavy state‑of‑the‑art methods.",3,3,4,3,2,2,3,4,4,4,3,3,3,3,3
Damian,DETR: End-to-End Object Detection with Transformers (Paper Explained),2020-05-28 15:09:01,https://youtu.be/T35ba_VXkMY,T35ba_VXkMY,UCZHmQk67mSJgfCCTn7xBfew,33431,Facebook has released a paper on end-to-end object detection with transformers. It uses first a CNN and then a transformer to detect objects and it does so via a bipartite matching training objective.,The daily transcript is a written version of each day's episode of CNN Tech. Use this transcript to help students with reading comprehension and vocabulary. Use the weekly Newsquiz to test your knowledge of stories you saw on CNNTech.,Discussed a high level overview of a capability built by facebook on how Object Detection with Transformers which using CNN. ,This paper does object detection in images using first a CNN and then a transformer to detect objects and it does so via a bipartite matching training objective.,"This is a very, very difficult task. So this paper proposes a super simple architecture, and will kind of go from the high level to the low to the implementation of each of the parts.","The tutorial explains how the DETR paper by Facebook AI researchers replaces complex, hand‑tuned object‑detection pipelines with a simple CNN‑backbone followed by a transformer encoder‑decoder that outputs a fixed‑size set of bounding‑box/tensor predictions. By using a bipartite (Hungarian) matching loss during training, the model learns to associate each predicted box with the correct ground‑truth annotation regardless of order, while also allowing a “nothing” class to discard unused predictions. This end‑to‑end approach eliminates many of the engineering hurdles of previous methods, offering a straightforward architecture that still handles varying numbers, sizes, and overlaps of objects.",3,1,4,4,3,4,2,5,4,2,3,2,4,3,1
Damian,Dreamer v2: Mastering Atari with Discrete World Models (Machine Learning Research Paper Explained),2021-02-19 16:11:18,https://youtu.be/o75ybZ-6Uu8,o75ybZ-6Uu8,UCZHmQk67mSJgfCCTn7xBfew,44548,"The world model is part of the dreamer v2 algorithm from the paper mastering Atari with discrete world models by Danny Scharhoffner, Timothy Lillicrop, Muhammad Nerozi, and Jimmy Ba of Google Brain, DeepMind","You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary. This is the paper, agent 57 outperforming the Atari human benchmark by DeepMind.","A video on Dreamer v2, an implementation of Reinforcement Learning and discusses how it was used to have the Dreamer v2 model play and win at atari games. ",Atari is one of the hardest games for reinforcement learning agents to solve.,"Dreamer v2 algorithm from the paper mastering Atari with discrete world models by Danny Scharhoffner, Timothy Lillicrop, Muhammad Nerozi, and Jimmy Ba of Google Brain, DeepMind and the University of Toronto","Dreamer v2 is a model‑based reinforcement‑learning algorithm that first learns a latent categorical world model of Atari games and then uses that model for actor‑critic training, enabling efficient simulation of future play. The model predicts dozens of future frames and agent actions with striking accuracy, outperforming single‑GPU model‑free rivals such as Rainbow, IQN",4,1,4,2,3,4,1,5,2,3,4,1,4,1,3
Matthew,Creating Stable Diffusion Interpolation Videos,2022-09-16 15:19:34,https://youtu.be/275_oeBw3vY,275_oeBw3vY,UCfzlCWGWYyIQ0aLC5w48gBQ,5716,Stable diffusion is put super simply a text to image generation AI which runs on only a few gigs of memory and which all by itself is absolutely insane. One of the more interesting things that we can do is interpolate between two or,DeepMind has created a visual grammar for image generation. The grammar can be used to create stunning images by using clip models. This video shows how to use the grammar to create a short summary.,"A short tutorial video introducing a new series on interpolation videos using Stable Diffusion, a text-to-image AI capable of generating diverse visuals with minimal hardware.",A new video in a new series on making interpolation videos with stable diffusion like the one playing right now.,I'll be using a package from Nate Raw on GitHub which makes this whole process a breeze to do.,"The video explains how to create smooth interpolation videos with Stable Diffusion using a GitHub package from Nate Raw, outlining a workflow that generates many images from prompts, manually selects the best ones, and then interpolates between them—optionally upscaling with ESRGAN. The speaker demonstrates with a Porsche 911 evolution example, encourages viewers to experiment with any images or themes, and notes the process runs on modest GPU memory. He also promotes a giveaway of an NVIDIA 3090 Ti GPU for attending the GTC conference and invites viewers to share their own interpolation projects.",3,3,3,2,2,4,4,3,2,2,3,3,4,2,2
Matthew,[Classic] ImageNet Classification with Deep Convolutional Neural Networks (Paper Explained),2020-07-23 13:00:13,https://youtu.be/Nq3auVtvd9Q,Nq3auVtvd9Q,UCZHmQk67mSJgfCCTn7xBfew,37944,"Alex Krzyzewski, Ilya Satskiwer and Jeffrey E. Hinton: ImageNet classification with deep convolutional neural networks. This paper is another one in the installment of our historical paper overview.",The daily transcript is a written version of each day's episode of Learning Intelligence 19. Use this transcript to help students with reading comprehension and vocabulary. Use the weekly Newsquiz to test your knowledge of stories you saw on Learning Intelligence.,"A video reviewing the landmark paper “ImageNet Classification with Deep Convolutional Neural Networks” by Krizhevsky, Sutskever, and Hinton, also known as AlexNet.","This paper was the first to show that you could actually use CUDA GPUs to train those large networks efficiently, and it won the ImageNet competition that year. And it did so by a very, very large margin. So kind","Alex Krzyzewski's paper on object recognition. It's a good read, and you'll get an idea of how much work has gone into it.","Alex Krzyzewski and colleagues explain that AlexNet—by using deep convolutional neural networks and CUDA‑accelerated GPUs—revolutionized image classification, allowing training on the vast ImageNet dataset (15 million images, 22 000 categories) and winning its 2012 competition by a huge margin. The talk emphasizes that the choice of a deep, structured architecture was driven by the need for immense model capacity and strong inductive biases (such as stationarity and locality) to handle the enormous, complex variability in real‑world images, a paradigm that shifted computer vision from hand‑crafted features to learned deep models.",2,2,4,2,3,2,2,4,3,3,2,3,4,3,2
Matthew,Building our Neural Network - Deep Learning and Neural Networks with Python and Pytorch p.3,2019-09-25 14:19:43,https://youtu.be/ixathu7U-LQ,ixathu7U-LQ,UCfzlCWGWYyIQ0aLC5w48gBQ,22933,In this tutorial we are going to be actually building our neural network and at least really basically explaining how we pass data. And then in the next tutorial we'll actually train the neural network.,"This week's episode of Learning Intelligence is going to be called Nanodegree. The first project is building a computer vision capstone. The second is an example of what it is I've been building. The third and final thing is if we go here, playground.tensorflow.org.","A tutorial video from a PyTorch deep learning series where the creator builds a feed-forward neural network from scratch using torch.nn and torch.nn.functional. The video covers defining layers, using ReLU activations, applying LogSoftmax for classification, and explaining how data flows through the network.",The next tutorial will teach you how to build a neural network.,Y we're gonna make a couple new imports So we want to import torch import torch dot nn as nnd And then we'll import,In this part of the tutorial the speaker explains how to build a plain,5,3,4,2,1,3,5,3,2,1,4,2,3,1,1
Matthew,Longformer: The Long-Document Transformer,2020-04-20 14:07:56,https://youtu.be/_8KNb5iqblE,_8KNb5iqblE,UCZHmQk67mSJgfCCTn7xBfew,20111,The long former is a variant of the transformer. It is a transformer that can deal with long documents. The goal is to actually just be able to put this entire document here into the model at the same time.,"The long former is a variant of the transformer that can deal with long documents. The classic transformers have a limit, a very harsh limit on the amount of tokens that they can look at at the same time. The long former has the encoder as a side input.","A video explaining Longformer: The Long Document Transformer by Beltagy, Peters, and Cohan from Allen AI. Yannic summarizes how Longformer modifies the standard transformer to handle longer text sequences efficiently by replacing full attention with a sliding window attention",The long document transformer is a variant of the transformer that can deal with long documents.,"I'm going to do this is I'll draw six units here. And the units are actually the input sequence. And they expose something called keys on the lower layer. And these are vectors that point somewhere, and the upper layer will produce what are called queries. And basically, the keys and queries, they decide where information gets routed to. Right?","The Longformer is a transformer variant that tackles the classic transformer’s quadratic‑time, limited‑context problem by replacing global self‑attention with a sliding‑window attention mechanism, allowing it to handle entire long documents in one pass. Unlike ordinary transformers that must split texts into independent chunks—losing cross‑chunk dependencies—the Longformer can attend to contiguous token windows, thus preserving long‑range context while keeping",4,4,5,3,2,4,4,5,2,1,4,4,4,3,2
Matthew,Jetson Nano 2GB from NVIDIA,2020-10-14 15:30:53,https://youtu.be/iRrySp4VoPs,iRrySp4VoPs,UCfzlCWGWYyIQ0aLC5w48gBQ,7623,The Jetson Nano is the little brother to the 4 gigabyte Nano the original one which we also reviewed on this channel some time ago. The only significant difference is this has 2 gigabytes of RAM This one has 4 the GPU is,"You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary. The Jetson Nano has 2 gigabytes of RAM and a 128 core Maxwell GPU.","A review video of the NVIDIA Jetson Nano 2GB, a smaller, cheaper version of the 4GB model designed for affordable AI and deep learning projects.","The Jetson Nano is a little brother to the 4 gigabyte Nano, which we also reviewed on this channel some time ago.",T this board I just see the one I'm not really sure you need two Two is kind of cool I guess so you could run like the two Raspberry Pi cameras and so on But if you need more cameras you can always,"The 2‑GB Jetson Nano is basically a trimmed‑down version of the 4‑GB model, sharing the same 128‑core Maxwell GPU and ARM CPU but with half the RAM, one CSI port, HDMI instead of DisplayPort, USB‑C power, and fewer USB ports for just $59. In practice, object‑detection workloads run almost as fast as on",3,1,5,3,1,3,1,5,2,1,3,1,5,2,1
Matthew,Meta-Learning through Hebbian Plasticity in Random Networks (Paper Explained),2020-08-12 9:20:25,https://youtu.be/v2GRWzIhaqQ,v2GRWzIhaqQ,UCZHmQk67mSJgfCCTn7xBfew,32259,"The paper is called meta learning through Hebbian plasticity in random networks by Elias Naharo and Sebastian Rizzi. It tries to implement this sort of more biologically plausible way of learning a policy adapting to the environment, and achieve","You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary. The goal of reinforcement learning agent is usually to maximize all the rewards that it collects.","A research video showing a totally different take on training agents, instead of using reinforcement learning, the authors build a system that learns how to learn using Hebbian plasticity.",This paper uses a policy that is adaptive during training to learn a policy that is able to walk forward.,"So this paper deals with this problem and tries to implement this sort of more biologically plausible way of learning a policy adapting to the environment, and achieve ultimately good performance in this task.","The video explains a recent paper on “meta‑learning through Hebbian plasticity in random networks,” which tackles quadruped locomotion not by training a fixed policy as in typical reinforcement learning, but by learning plasticity rules that adapt a randomly initialized neural network during each episode. In this approach, the network weights are not fixed after training; instead, the Hebbian learning rules continually update the network in response to sensory input, allowing the robot to re‑configure itself on the fly, even when legs are damaged. This biologically inspired method demonstrates rapid adaptability and robustness to perturb",4,1,4,2,3,5,1,4,3,3,3,1,5,3,3
Matthew,"Machine Learning PhD Survival Guide 2021 | Advice on Topic Selection, Papers, Conferences &amp; more!",2021-03-30 14:09:13,https://youtu.be/rHQPBqMULXo,rHQPBqMULXo,UCZHmQk67mSJgfCCTn7xBfew,13548,"Train. on how to do a PhD. So mainly that you don't repeat my mistakes. When it comes down to choosing a topic, make sure you look for something that your advisor or the senior people around you have lots of experience","You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary.Limit your summary to 5 sentences.","A video about how to survive a machine learning PhD, with advice on picking a niche topic, avoiding hype, and writing papers that give reviewers nothing to reject.",The podcast is a guide to survive a machine learning PhD in 2021.,Choose a topic that matches your interests. Find a subject that fits your expertise. Find niche topics. Look for smaller communities that publish year after year. Write papers that cannot be rejected.,"The speaker offers practical PhD advice, stressing that a successful topic should align with your advisor's expertise, your interests, and your existing skills while also being realistic about available resources.  They warn against chasing the most hyped subjects, suggesting instead to pursue niche, low competition areas where you can become a leading expert.  For publishing, the key is to write â€œunrejectionable papers learn from top papers in your niche, eliminate obvious criticisms, and treat rough reviews as a normal, non personal hurdle rather than a failure.",5,1,5,4,4,5,1,5,4,4,5,1,5,1,4
Matthew,Streamlit for ML #3 - Make Apps Fast with Caching,2022-01-27 15:00:36 UTC,https://youtu.be/lYDiSCDcxmc,lYDiSCDcxmc,UCv83tO5cePwHMt1952IVVHw,6980,"In this video, we are going to continue with the app that we've been building. We have this AI Q&A, it's just the very basics of Streamlit. And we have this search bar or text input that we","Streamlit is an app that uses machine learning and natural language understanding to help people with information retrieval problems. In this video, we look at the basics of the app, including the search bar and text input. We also look at how we can iterate through all the contexts that we returned from Pinecone.",A video about optimizing a Streamlit-based AI Q&A app by fixing slow performance caused by reloading ML models and Pinecone connections every time the UI changes.,"In this video, we continue with the app that we've been building.","We're going to have a look at how we can iterate through all the contexts that we returned from Pinecone and then display them. Now at the moment, it's very ugly and at the same time, another really bad thing that we need to solve in this video is this takes forever to do anything.","The video explains how the presenter refactors a Streamlit‑based AI Q&A app to avoid re‑downloading the retrieval model and reconnecting to Pinecone on every rerun. After discovering that st.cache re‑hashes API connections and slows execution, they switch to st.experimental_singleton, which runs each expensive operation only once and keeps the result cached for the session’s lifetime. The final demo shows a single‑execution spinner and validates that the app now loads quickly and without repeated expensive initializations.",3,3,4,2,4,3,3,4,1,2,4,4,4,1,3
Matthew,How to avoid knee surgery: Strengthen your glute medius (here&#39;s how),2020-04-13 9:45:30,https://youtu.be/qmPfQOU7aHU,qmPfQOU7aHU,UCr8O8l5cCX85Oem1d18EezQ,9612,"When rona used to be a weight lifter or bodybuilder and whatever and I used to lift heavy weight every basically every session, I found myself I ran into a lot of lower back injuries. I learned that my glutes didn",Use the weekly Newsquiz to test your knowledge of stories you saw on Raw. Use the Daily Discussion to help people understand today's featured news stories.,"A video about glute activation for heavy lifting, focused on prepping the body to avoid lower back strain by properly engaging the glutes.",The theme for today is preparation.,"So I'm going to start using my glutes first. So let's get on the ground. It's gonna be completely ground-based. There's no rest. We're just gonna go one leg and then into the other leg. And we're doing 10 reps per movement. By the end, you should feel a little pump in your butt cheeks.","After years of heavy lifting that led to lower‑back spasms, the narrator discovered that weak glute activation was causing his back to over‑compensate. He now starts every session with a ground‑based glute‑mobilizing routine that alternates each leg through forward‑back movements, clockwise and counter‑clock",4,3,4,2,4,3,3,5,1,2,3,3,3,1,2
Matthew,Setup Apple Silicon Mac for Machine Learning in 13 minutes (TensorFlow edition),2021-12-05 11:00:18,https://youtu.be/_1CaUOHhI6U,_1CaUOHhI6U,UCr8O8l5cCX85Oem1d18EezQ,11818,"The M1 machine learning cooking show. The code in this video will work for the M1, M1 Pro and M1 Max. We're gonna go ahead and set up an environment or get our machines ready for machine learning.","This is a fast-paced tutorial on how to build a neural network using Python and TensorFlow. The code in this video will work for the M1, M1 Pro and M1 Max. To run the demo you do need a GPU that is CUDA capable though the code is available.",A video about setting up machine learning on a new M1 Pro MacBook using TensorFlow with GPU acceleration via Apple’s Metal framework.,The M1 machine learning cooking show teaches you how to use the new M1 Pro and M1 Max machines for machine learning.,Python 3.9.,"The video walks viewers through setting up a machine‑learning environment on an M1‑powered MacBook Pro, highlighting that the code works for M1, M1 Pro, and M1 Max chips. It starts by installing Homebrew, then Miniforge (an ARM‑compatible conda installer), and shows how to create a dedicated conda environment for TensorFlow projects. Throughout, the speaker explains package‑manager basics and emphasizes using isolated environments to keep dependencies clean for data‑science tools.",4,4,5,4,1,5,5,4,4,1,4,5,5,3,1