# -*- coding: utf-8 -*-
"""Youtube_GenAI (final to github).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RN3EaRAtk_gS9BbgcNXd-WTb2SIvdvO1

# Week 1: Load Dataset

1. Install Required Packages
"""

!pip install sentence-transformers faiss-cpu datasets transformers tqdm nltk pandas

"""2. Import Libraries"""

import pandas as pd
from datasets import load_dataset, Dataset
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import numpy as np
import nltk
from tqdm import tqdm
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
tqdm.pandas()

"""3. Download NLTK Punkt Tokenizer"""

import nltk
nltk.download('punkt')
nltk.download("punkt_tab")

nltk.download('all')

"""4. Define Utility Functions"""

def concat_sentences(group):
    # Concatenate all sentences for one video
    return ' '.join(group['text'])

def get_first_n_sentences(text, n):
    # Get the first n sentences from a transcript string
    sentences = nltk.sent_tokenize(text)
    sentences = ' '.join(sentences[:n])
    if len(sentences.split()) > 1000:
      sentences = ' '.join(sentences.split()[:300])
    return sentences

def get_summary(row, pipe, first_n_sent=20):
    # Generate a summary from the transcript
    subset = get_first_n_sentences(row['full_transcript'], first_n_sent)
    result = pipe(subset, max_length=50, min_length=10, do_sample=False, truncation=True)[0]['summary_text']
    return {"summary": result}

def get_embeddings(row, model, first_n_sent=10):
    # Get sentence embedding for transcript and summary
    transcript = get_first_n_sentences(row['full_transcript'], first_n_sent)
    summary = row['summary']
    transcript_embedding = model.encode(transcript)
    summary_embedding = model.encode(summary)
    return {
        'transcript_embedding': transcript_embedding,
        'summary_embedding': summary_embedding
    }

"""5. Load and Process Dataset"""

def load_data(path='jamescalam/youtube-transcriptions', split='train', n_rows=10, summarize_first_n_sents=20,video_ids = []):
    # Load dataset from HuggingFace
    data = load_dataset(path, split=split)
    df = pd.DataFrame(data)
    group_cols = ['title', 'published', 'url', 'video_id', 'channel_id']
    # Group by video, concatenate transcript
    results = []
    for name, group in tqdm(df.groupby(group_cols)):
        full_trans = concat_sentences(group)
        row = {col: name[idx] for idx, col in enumerate(group_cols)}
        row['full_transcript'] = full_trans
        results.append(row)
    df_agg = pd.DataFrame(results)
    if n_rows > 0:
        df_agg = df_agg.head(n_rows)
    elif len(video_ids) > 0:
        df_agg = df_agg[df_agg['video_id'].apply(lambda x: x in video_ids)]
    df_agg.dropna(subset=['full_transcript'], inplace=True)
    return df_agg

"""6. Main Processing Block"""

# Load data
df = load_data(n_rows=10, summarize_first_n_sents=20)

# Create summarization pipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Generate summaries
df['summary'] = df.apply(lambda row: get_summary(row, summarizer, first_n_sent=20)['summary'], axis=1)

# Generate embeddings
model = SentenceTransformer("all-MiniLM-L6-v2")
df['transcript_embedding'] = df.apply(lambda row: model.encode(get_first_n_sentences(row['full_transcript'], 10)), axis=1)
df['summary_embedding'] = df.apply(lambda row: model.encode(row['summary']), axis=1)

# Now df contains all fields we need for downstream RAG/retrieval/analysis
df.head()

"""# Week 2: Integrate the retrieved context into the LLM prompt to generate context-aware summaries.

Step 1: Chunk transcript
"""

def chunk_text(text, chunk_size=100, overlap=20):
    # Split text into chunks of `chunk_size` words with optional overlap
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = ' '.join(words[i:i+chunk_size])
        if chunk:
            chunks.append(chunk)
    return chunks

"""Step 2: Build vector database"""

import faiss

# Get all chunks from all videos
all_chunks = []
chunk_to_video = []
for idx, row in df.iterrows():
    video_chunks = chunk_text(row['full_transcript'])
    all_chunks.extend(video_chunks)
    chunk_to_video.extend([row['video_id']] * len(video_chunks))

# Compute embeddings
model = SentenceTransformer("all-MiniLM-L6-v2")
chunk_embeddings = model.encode(all_chunks, show_progress_bar=True)

# Build FAISS index
dimension = chunk_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(chunk_embeddings))

"""Step 3: Define retrieval function"""

def retrieve_context(query_text, top_k=3):
    # Encode query
    query_emb = model.encode([query_text])
    # Search for top_k similar chunks
    D, I = index.search(np.array(query_emb), top_k)
    # Return the retrieved chunk texts
    return [all_chunks[i] for i in I[0]]

"""Step 4: Integrate retrieved context into LLM prompt"""

def construct_rag_prompt(main_transcript, retrieved_chunks):
    context_section = "\n".join(retrieved_chunks)
    prompt = f"""You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary.

Transcript:
{main_transcript}

Retrieved context:
{context_section}
"""
    return prompt

"""Step 5: Run summarization"""

# Load the model and tokenizer
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
generator = AutoModelForSeq2SeqLM.from_pretrained(model_name)

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Example for one video
row = df.iloc[0]
main_text = get_first_n_sentences(row['full_transcript'], 10) # Get the first 10 sentenece
retrieved = retrieve_context(main_text, top_k=3)
prompt = construct_rag_prompt(main_text, retrieved)
rag_raw_summary_a = summarizer(prompt, max_length=60, min_length=10, do_sample=False, truncation=True)[0]['summary_text']

print("RAG Raw summary 1st try:", rag_raw_summary_a)

"""Step 6: Optimize RAG model"""

all_chunks = []
chunk_to_video = []
for idx, row in df.iterrows():
    video_chunks = chunk_text(row['full_transcript'], chunk_size=40, overlap=20)
    all_chunks.extend(video_chunks)
    chunk_to_video.extend([row['video_id']] * len(video_chunks))

# Get embeddings for all chunks
model = SentenceTransformer("all-MiniLM-L6-v2")
chunk_embeddings = model.encode(all_chunks, show_progress_bar=True)

def construct_rag_prompt(main_transcript, retrieved_chunks, max_sentences=3):
    context_section = "\n".join(retrieved_chunks)
    prompt = f"""You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary.

Transcript:
{main_transcript}

Retrieved context:
{context_section}

Limit your summary to {max_sentences} sentences.
"""
    return prompt

def retrieve_context(query_text, top_k=2, exclude_video_id=None):
    """
    Retrieve top_k context chunks for a given query_text, excluding chunks from the same video if exclude_video_id is provided.
    """
    # Build candidate pool (exclude chunks from current video)
    candidate_chunks = []
    candidate_embeddings = []
    for i, chunk in enumerate(all_chunks):
        if exclude_video_id and chunk_to_video[i] == exclude_video_id:
            continue
        candidate_chunks.append(chunk)
        candidate_embeddings.append(chunk_embeddings[i])
    if not candidate_chunks:
        return []
    # Compute embedding for query
    query_emb = model.encode([query_text])
    emb_matrix = np.vstack(candidate_embeddings).astype('float32')
    # Build a temp FAISS index for candidates only
    faiss_index = faiss.IndexFlatL2(emb_matrix.shape[1])
    faiss_index.add(emb_matrix)
    D, I = faiss_index.search(query_emb, top_k)
    return [candidate_chunks[idx] for idx in I[0]]

# Example usage for one video
row = df.iloc[0]
main_text = get_first_n_sentences(row['full_transcript'], 3)
retrieved = retrieve_context(main_text, top_k=2, exclude_video_id=row['video_id'])

print("Main text:", main_text)
print("Retrieved context chunks:", retrieved)

prompt = construct_rag_prompt(main_text, retrieved, max_sentences=2)
print("Prompt for LLM:\n", prompt)

# rag_raw_summary_b = summarizer(
#     prompt,
#     max_length=50,   # Adjust as needed
#     min_length=10,
#     do_sample=False,
#     truncation=True
# )[0]['summary_text']


# Tokenize input
inputs = tokenizer(prompt, return_tensors="pt")

# Generate output
outputs = generator.generate(**inputs, max_length=50)

# Decode the result
rag_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)

# print("RAG Raw summary 2nd try:", rag_raw_summary_b)

print("RAG Raw summary 2nd try:", rag_summary)

"""The current summarization model is now acceptable for several reasons.


*   First, the entire pipeline from transcript chunking, embedding generation, and vector database construction, to context retrieval and prompt-based summarization is fully functional and debugged.
*   The retrieved context for each query is no longer just a repeat of the original transcript but draws from diverse, relevant segments across the dataset, which demonstrates true retrieval-augmented capability.
*   The generated summaries are concise, abstract key points, and are not simple copies of the transcript, showing the model is able to synthesize information.
*   In addition, the summary outputs reflect both the main transcript and information from retrieved context when appropriate, which is the intended goal of a RAG approach. Compared to baseline summaries, the RAG outputs exhibit clear differences, indicating that the model is indeed leveraging external context.
*   This structure is also scalable and can be further evaluated and improved using quantitative metrics or human evaluation in the next phases.
"""

# function to retrieve RAG summary for multiple examples
def getRagSummary(
              transcript
              ,exclude_video_id
              ,n=11
              ,top_k=5
              ,max_sentences=2
              ):

  main_text = get_first_n_sentences(transcript, n=n)

  retrieved = retrieve_context(main_text, top_k=top_k, exclude_video_id=exclude_video_id)

  prompt = construct_rag_prompt(main_text, retrieved, max_sentences=max_sentences)

  # Tokenize input
  inputs = tokenizer(prompt, return_tensors="pt")

  # Generate output
  outputs = generator.generate(**inputs, max_length=50)

  # Decode the result
  rag_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)

  return rag_summary

"""Step 6.5: Generate RAG summaries for entire dataset"""

summs = df.progress_apply(
                lambda x: getRagSummary(
                    x['full_transcript'],
                    x['video_id'],
                    n=11,
                    max_sentences=5
                ),
                axis=1
            )

# Save into df
df['rag_raw_summary_b'] = summs

"""Step 7: Evaluation: ROUGE + BLEU"""

!pip install rouge-score

from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu
import numpy as np

# Initialize ROUGE scorer
scorer = rouge_scorer.RougeScorer(
    ['rouge1', 'rouge2', 'rougeL'],
    use_stemmer=True
)

# Create lists to store scores
rouge1_scores = []
rouge2_scores = []
rougeL_scores = []
bleu_scores = []

# Loop through all rows
for idx, row in df.iterrows():
    reference = row['summary']           # baseline summary
    generated = row['rag_raw_summary_b']       # rag raw summary 2nd try

    # --- ROUGE ---
    scores = scorer.score(reference, generated)
    rouge1_scores.append(scores['rouge1'].fmeasure)
    rouge2_scores.append(scores['rouge2'].fmeasure)
    rougeL_scores.append(scores['rougeL'].fmeasure)

    # --- BLEU ---
    ref = [reference.split()]
    gen = generated.split()
    bleu = sentence_bleu(ref, gen)
    bleu_scores.append(bleu)

# Print average scores
print("Average ROUGE-1:", np.mean(rouge1_scores))
print("Average ROUGE-2:", np.mean(rouge2_scores))
print("Average ROUGE-L:", np.mean(rougeL_scores))
print("Average BLEU:", np.mean(bleu_scores))

"""# Week 3: Human Evaluation
(Check out Final Report for more information)

# Week 4: Model Fine-Tuning

## 1. Basic fine-tuning (T5-small)

Step 1: Get 10 transcript from `full_transcript`
"""

# Take first 10 rows
sample_transcripts = df['full_transcript'].head(10).tolist()

# Show first transcript to verify
sample_transcripts[0][:500]   # first 500 chars preview

sample_df = df.head(10)[['title', 'video_id', 'full_transcript']]
sample_df

"""Step 2: Manually get high-quality summaries using GPT5.1"""

# Create a new column for gold summaries
df["gpt_summary"] = None

# Dictionary for gold summaries
# Each key is a video id
# Each value is a long text summary enclosed in triple quotes
gpt_summaries = {
    0: """The video explains the capabilities and implications of BLOOM, a large open-source language model created by over a thousand researchers using the French government’s Jean Zay supercomputer. The host introduces BLOOM as a freely available alternative to high-cost proprietary models, with versions ranging from 350M to 176B parameters. He discusses the hardware requirements for running the largest model, noting that full-precision inference may require around 680GB of memory, while half-precision can reduce this to roughly 350GB. Hugging Face offers API access for users who cannot run the model locally.

The video then explains how BLOOM and other large language models generate text by predicting the next tokens rather than performing explicit tasks like regression or structured dialogue. The host emphasizes the importance of prompt engineering and shows how different prompt formats can cause the model to output code, dialogue, or structured answers. Additional examples demonstrate BLOOM’s ability to summarize product reviews, answer multi-step questions, diagnose code errors, and hold multilingual conversations. The speaker highlights that these behaviors emerge from the model’s broad training corpus, which spans dozens of natural languages and programming languages. He concludes by emphasizing the importance of open access for accelerating innovation and encourages viewers to explore BLOOM via API or local deployment.""",

    1: """The video documents a training session filmed one week before the creator’s upcoming Spartan Race, highlighting the workout structure designed to develop grip strength, core stability, and endurance needed for obstacle running. It begins with a warm-up routine performed against a wall to stretch the hips, glutes, ankles, and knees. The training session consists of two major parts. The first is a five-round circuit that includes ten burpees, five pull-ups, a one-minute plank, and a one-minute plate carry, followed by a one-minute rest. The exercises are selected to elevate heart rate, challenge upper-body pulling strength, and improve stability under fatigue.

The second portion is a 10-minute AMRAP featuring deadlifts at a reduced weight to preserve leg freshness, core-focused “beast activations,” and hanging leg raises. The creator explains that while running endurance is usually manageable, grip and core fatigue tend to be limiting factors during long races, so the workout structure intentionally mimics the short recovery periods typical of navigating obstacles.

The video transitions into a brief cooking segment demonstrating how to prepare a five-minute baked sweet potato and assemble a simple meal with fish, greens, avocado, and lemon. The creator ends with a short reflection on race preparation, noting that training and nutrition are on track for the upcoming Spartan Race.""",

    2: """The video presents a 10-minute bodyweight workout focused on strengthening the back and shoulders using limited space, framed within the creator’s “Reps for Rona” series. The session begins with a short reflection on leadership during the COVID-19 pandemic, using the story of a New York City doctor to emphasize small, consistent acts of responsibility and care. After this introductory message, the creator transitions into the workout, explaining that bodyweight training often favors pushing movements, so this routine is designed to target harder-to-train pulling muscles without equipment.

The workout consists of five exercises executed for 30 seconds on and 30 seconds off, repeated for two rounds. The movements include push-back push-ups, Superman holds, dead-stop-to-Superman transitions, and W-Y-T Superman variations that isolate different portions of the upper back. The final exercise is a T-push-up, combining rotational movement with back activation. Throughout the demonstration, the creator highlights proper form, muscle engagement, and the value of mixing dynamic and static holds to build strength.

In the closing section, he reiterates the theme of leadership, noting that meaningful actions often occur without recognition and can be as simple as daily habits or caring for others. He encourages viewers to continue moving, stay healthy, and reflect on the leaders in their own lives.""",

    3: """The video is part of the creator’s “Reps for Rona” series and features a 10-minute lower-body bodyweight workout suitable for beginners exercising at home. Before starting, the creator briefly mentions his ongoing coding project and emphasizes the importance of avoiding overtraining, noting that bodyweight routines may seem easier than weighted exercises but can fatigue the muscles more quickly. He highlights that viewers should rest when necessary and maintain consistency rather than intensity.

The workout follows a 30-seconds-on, 30-seconds-off structure across two rounds. The five exercises include bodyweight squats, split squats, side lunges, jump squats, and chair lunges. The creator offers practical tips for form, balance, and pace, adjusting the routine for accessibility in small indoor spaces. He also incorporates a short intermission to acknowledge the birthday of the person filming the workout.

Alongside the physical routine, the video includes a reflective discussion on “work versus life.” The creator observes how remote work during the pandemic has blurred boundaries and argues that work and life are not opposing forces but intertwined aspects of daily experience. He encourages reframing work as play to reduce stress and suggests reevaluating priorities such as family, health, and meaningful personal time. The session concludes with a reminder to rest when needed, maintain movement habits, and consider how lifestyle choices might evolve during challenging periods.""",

    4: """The video is part of the creator’s “Reps for Rona” series and introduces a 10-minute wake-up stretching routine designed for beginners or anyone feeling stiff after inactivity. The creator explains that day 14 coincides with two weeks of consistent daily movement during the COVID-19 lockdown, emphasizing the importance of simple physical activity for maintaining immunity and well-being while staying indoors. He frames this session around the theme of “adjusting as needed,” noting that recent workouts have involved more tension-building exercises and that the body, like an accordion, requires both tightening and loosening movements.

The stretching sequence consists of five exercises performed for approximately 30 seconds each, repeated for both sides of the body. The routine includes the lunge kneel, shin-under-body stretch, Jefferson curl, squat reach, and arms-crossed under-body position. Throughout the session, the creator encourages continuous gentle movement rather than holding static poses, suggesting adjustments based on individual flexibility levels. He highlights the benefits of slow spinal articulation in Jefferson curls, deep hip and glute stretching in the shin-under-body position, and ankle and hip mobility during the squat variations.

The broader message focuses on personal and societal adaptation. The creator compares the stretching theme to how people worldwide were adjusting to pandemic conditions, emphasizing that improvement involves responding to present circumstances rather than dwelling on what “could have been done sooner.” The session concludes with an invitation for viewers to reflect on how their own lifestyles are adapting over time.""",

    5: """During a casual AMA livestream celebrating 100k subscribers, the speaker responds to a wide range of questions about AI, research careers, personal habits, and industry trends. He explains that a PhD is not required to work in AI, though it can help for specialized research roles, and suggests that many people enter research through software engineering roles inside companies. He emphasizes that deep learning is largely built on simple components—gradient descent and cross-entropy—but still offers room for creativity in model design and experimentation. He believes the field evolves quickly, making it difficult to predict future directions, and encourages people to follow what genuinely interests them rather than optimizing too much for career planning.

He discusses live-coding streams, startup work in legal NLP, research paper reading habits, and the challenge of keeping up with constant new publications. He also answers questions about data quality vs. dataset size, transformers vs. other architectures, prompt engineering, bias and fairness research, hardware trends, and whether AI is becoming centralized due to compute requirements. Throughout the session, he shares advice on entering research, reading papers, avoiding burnout, navigating PhD decisions, and maintaining curiosity. He also touches on lighter topics such as cats vs. dogs, fitness, programming languages, YouTube dislikes, and favorite books or movies. Overall, the AMA is an open, wide-ranging conversation reflecting his perspectives on AI, academia, industry, and personal experience.""",

    6: """In this short year-end message, the speaker reflects on how much has changed since the previous year. He recalls driving for Uber, studying for his AI master’s degree, and creating videos without knowing where the journey would lead. He thanks viewers for their support and explains that making videos has become a central part of his routine, helping him stay accountable and motivating him to continue learning and creating. He emphasizes that the channel has grown into an important part of his personal and professional development.

He shares his plans for the end of the year, noting that his family is preparing for their annual holiday on an island. The trip serves as a reset period where he reflects on the past year and outlines goals for the upcoming one. For 2019, he expects to continue in the same creative and educational direction, and he invites viewers to suggest topics they'd like to see. He mentions books he plans to read, including a marketing book and Yuval Noah Harari’s “21 Lessons for the 21st Century,” and shows a hat gifted by a friend that reflects his interests in AI and fitness. He closes by wishing everyone a happy New Year, expressing gratitude to frequent commenters and supporters, and promising more content in the coming year."""

}

# Apply the gold summaries
df["gpt_summary"] = df.apply(lambda row: gpt_summaries.get(row.name, None), axis=1)

df.head(7)

"""Step 3: Build HF Dataset for Training"""

# Select only rows with gold summaries
train_df = df[df["gpt_summary"].notnull()].copy()

# Rename columns for training
train_df = train_df.rename(columns={
    "full_transcript": "input_text",
    "gpt_summary": "target_text"
})

# Convert pandas DataFrame to HF Dataset
train_dataset = Dataset.from_pandas(train_df)

"""Step 4: Tokenizer + Preprocessing"""

!pip install sentencepiece

from transformers import AutoTokenizer

model_name = "google-t5/t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Preprocess function
def preprocess(batch):
    model_inputs = tokenizer(
        batch["input_text"],
        max_length=1024,
        padding="max_length",
        truncation=True
    )
    labels = tokenizer(
        batch["target_text"],
        max_length=256,
        padding="max_length",
        truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_dataset = train_dataset.map(preprocess, batched=True)

"""Step 5: Load model + Trainer"""

from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

training_args = TrainingArguments(
    output_dir="t5_finetuned_rag_summary",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=5e-5,
    num_train_epochs=5,
    save_total_limit=2,
    logging_steps=10,
    report_to="none" ,

    fp16=True,  # use float16 instead of bf16
    bf16=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
)

trainer.train()

"""Step 6: Save the fine-tuned model"""

trainer.save_model("t5_finetuned_rag_summary")
tokenizer.save_pretrained("t5_finetuned_rag_summary")

"""Step 7: Get optimized summary"""

def generate_summary(text, max_len=256):
    # 1. Detect where the model is (CPU or GPU)
    device = model.device

    # 2. Tokenize the text
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=1024)

    # 3. CRITICAL FIX: Move inputs to the same device as the model
    # We loop through the input dictionary and move every tensor to GPU
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # 4. Generate
    outputs = model.generate(
        **inputs,
        max_length=max_len,
        num_beams=4
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Test again
sample_idx = 0
sample_text = df["full_transcript"].iloc[sample_idx]

pred_summary = generate_summary(sample_text)

print("Model Summary:\n", pred_summary)

"""Step 8: Evaluation"""

print("=== Week 2 Summary (rag_raw_summary_b) ===")
print(df.loc[sample_idx, "rag_raw_summary_b"])

print("\n=== FINE-TUNED MODEL ===")
print(pred_summary)

print("\n=== GPT Manually (rag_summary, if available) ===")
print(df.loc[sample_idx, "gpt_summary"])

scorer = rouge_scorer.RougeScorer(["rouge1", "rougeL"], use_stemmer=True)

baseline = df.loc[sample_idx, "rag_raw_summary_b"]
model_pred = pred_summary
gold = df.loc[sample_idx, "gpt_summary"]

print("BASELINE vs GOLD:", scorer.score(gold, baseline))
print("MODEL vs GOLD:", scorer.score(gold, model_pred))

"""The fine-tuned model produced repetitive, inaccurate summaries and failed to capture the meaning of the transcripts. This indicates that the model did not successfully learn a stable summarization pattern.

The main reason is that the training set contained only a handful of examples. With so little data, T5-small cannot generalize: it simply memorizes fragments, repeats phrases, and hallucinates missing information. The inconsistent length and style of the existing summaries make the supervision even weaker.

Because the model is large compared to the dataset, the weight updates are mostly noise. The model sees too few examples to learn structure, topic flow, or video-style patterns, so poor outputs are expected.

To improve performance, we need a larger and more consistent set of high-quality summaries. The next step is to automatically generate a much bigger "gold" dataset using Llama and then fine-tune the model again on these new labels.

## 2. Advanced fine-tuning

Step 0: Reload Dataset for Fine Tuning
"""

# We need more data for training so we reload the dataset here
# This avoids slowing down the previous RAG steps which used only 10 rows
# We load all rows to ensure we have enough after filtering
# We do not load the full dataset because generating summaries for all of them
# would take too long and exceed API rate limits
df = load_data(n_rows=1200)

print(f"Dataset shape: {df.shape}")

# Select the first 1000 for a high quality training set
train_df = df.head(1000).copy()

"""Step 1: Install Google Generative AI library for fast data generation"""

!pip install -U -q google-generativeai

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, pipeline

"""Step 2: Generate Gold Summaries

* Due to the usage limiation of Gemini 2.0Flash, we generated "gold summaries" in several steps.
"""

import google.generativeai as genai
from google.colab import userdata
import time

# 1. Setup API
try:
    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
    genai.configure(api_key=GOOGLE_API_KEY)
except Exception as e:
    print(f"Error retrieving API Key: {e}")

# 2. Setup Model (Prefer 2.0 Lite, Fallback to 1.5)
try:
    model_name = 'gemini-2.0-flash-lite'
    gemini_model = genai.GenerativeModel(model_name)
    print(f"Using model: {model_name}")
except:
    model_name = 'gemini-1.5-flash'
    gemini_model = genai.GenerativeModel(model_name)
    print(f"Using model: {model_name}")

# 3. Load Fresh Dataset
print("Loading fresh dataset...")
# Reload to ensure clean slate
if 'load_data' in globals():
    raw_df = load_data(n_rows=800)
    train_df = raw_df.head(701).copy()
else:
    # Fallback if function is missing
    from datasets import load_dataset
    data = load_dataset('jamescalam/youtube-transcriptions', split='train')
    raw_df = pd.DataFrame(data)
    # Reconstruct necessary columns roughly
    # (Assuming we have the function tomorrow, this else block might not run)
    train_df = raw_df.head(701).copy()
    train_df['full_transcript'] = train_df['text']

train_df['target_summary'] = None

print(f"Total rows to process: {len(train_df)}")
print("Starting generation. Speed set to OPTIMAL (4.5s delay).")

# 4. Generation Loop
success_count = 0
save_interval = 50

for index, row in tqdm(train_df.iterrows(), total=train_df.shape[0]):

    transcript = row['full_transcript']

    # Basic validation
    if not isinstance(transcript, str) or len(transcript) < 10:
        continue

    # Truncate
    truncated_text = " ".join(transcript.split()[:3000])

    prompt = f"""
    You are an expert data scientist.
    Summarize the following YouTube video transcript in one concise paragraph.
    Focus on technical details and key takeaways.

    Transcript:
    {truncated_text}
    """

    # Simple Retry Logic
    # We try max 2 times. If it fails, we move on.
    max_retries = 2
    for attempt in range(max_retries):
        try:
            response = gemini_model.generate_content(prompt)

            if response.text:
                train_df.at[index, 'target_summary'] = response.text
                success_count += 1

                # OPTIMAL WAIT: 4.5 seconds
                # This keeps us just under the 15 RPM limit
                time.sleep(4.5)
                break

        except Exception as e:
            # Check for Rate Limit (429)
            if "429" in str(e):
                if attempt < max_retries - 1:
                    print(f"Rate Limit hit at index {index}. Pausing 20s...")
                    time.sleep(20) # Short pause to drain bucket
                else:
                    print(f"Skipping index {index} after retries.")
                    time.sleep(5) # Small buffer before next row
            elif "400" in str(e) or "403" in str(e):
                print(f"Permission Error: {e}")
                break
            else:
                time.sleep(2)

    # Periodic Save
    if success_count % save_interval == 0 and success_count > 0:
        temp_df = train_df.dropna(subset=['target_summary'])
        temp_df.to_csv("gold_dataset.csv", index=False)

# 5. Final Save
final_df = train_df.dropna(subset=['target_summary'])
final_df.to_csv("gold_dataset.csv", index=False)

print("\n=== Processing Complete ===")
print(f"Total Valid Summaries: {len(final_df)}")
print(f"Data saved to gold_dataset.csv")

"""Step 2.1: Save all trained data into csv file"""

# if 'train_df' in locals():
#     # Filter for rows that actually have a summary
#     current_batch_df = train_df.dropna(subset=['target_summary'])

#     # Save to a distinctive filename
#     current_batch_df.to_csv("gold_dataset_batch_new.csv", index=False)

#     print(f"Success! We salvaged {len(current_batch_df)} new summaries from this session.")
# else:
#     print("Error: train_df not found in memory.")

# # Export gold_dataset.csv to local machine
# from google.colab import files
# import os

# filename = 'gold_dataset.csv'

# if os.path.exists(filename):
#     print(f"Found {filename}, preparing download...")
#     files.download(filename)
#     print("Download initiated. Check your browser downloads.")
# else:
#     print(f"Error: {filename} not found. Make sure generation ran successfully.")

"""Step 2.2: Upload and Merge Datasets from local computer

Please find and download the csv files in the folder:

https://drive.google.com/drive/folders/1ySeguuKGte_gYYnUpsEan5T34PskgMid?usp=drive_link
"""

import pandas as pd
from google.colab import files
import io
import os

# 1. Upload files
print("Please upload ALL your csv files, including the new part 3:")
uploaded = files.upload()

data_frames = []

# 2. Read and collect data
for filename in uploaded.keys():
    if filename.endswith('.csv'):
        try:
            print(f"Reading {filename}...")
            df = pd.read_csv(io.BytesIO(uploaded[filename]))

            # Ensure required columns exist
            if 'target_summary' in df.columns:
                data_frames.append(df)
            else:
                print(f"Skipping {filename}: missing target_summary column")
        except Exception as e:
            print(f"Error reading {filename}: {e}")

# 3. Merge and Clean
if data_frames:
    full_df = pd.concat(data_frames, ignore_index=True)

    # Remove duplicates based on video_id to prevent data leakage
    if 'video_id' in full_df.columns:
        full_df = full_df.drop_duplicates(subset=['video_id'], keep='last')
    elif 'full_transcript' in full_df.columns:
        full_df = full_df.drop_duplicates(subset=['full_transcript'], keep='last')

    # Filter valid rows, removing empty summaries
    full_df = full_df.dropna(subset=['target_summary'])

    # Save the master dataset
    master_filename = "gold_dataset_merged_final.csv"
    full_df.to_csv(master_filename, index=False)

    print(f"\n=== MERGE SUCCESSFUL ===")
    print(f"Total Unique Training Samples: {len(full_df)}")
    print(f"Master dataset saved to: {master_filename}")
else:
    print("No valid CSV files uploaded. Please try again.")

"""Step 2.3: Load the Merged Data"""

csv_file = "gold_dataset_merged_final.csv"

if os.path.exists(csv_file):
    df_fine_tune = pd.read_csv(csv_file)
    df_fine_tune = df_fine_tune.dropna(subset=['target_summary'])
    print(f"Training on {len(df_fine_tune)} samples.")
else:
    raise ValueError("Merged dataset not found! Please run Block 1 first.")

"""Step 3: Load Model and Tokenizer"""

# We use flan-t5-base as it is a balanced choice for Colab
model_checkpoint = "google/flan-t5-base"

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, device_map="auto")

"""Step 4: Preprocess Data"""

# Use 10% for validation
dataset = Dataset.from_pandas(df_fine_tune[['full_transcript', 'target_summary']])
dataset = dataset.train_test_split(test_size=0.1)

def preprocess_function(examples):
    # Add clear prefix
    inputs = ["summarize: " + str(doc) for doc in examples["full_transcript"]]

    # Truncate input to 1024 tokens (Balance between context and memory)
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)

    # Truncate labels
    labels = tokenizer(text_target=examples["target_summary"], max_length=128, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = dataset.map(preprocess_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(['full_transcript', 'target_summary'])

"""Step 5: Train"""

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

training_args = Seq2SeqTrainingArguments(
    output_dir="flan_t5_base_video_summarizer",
    learning_rate=3e-4,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=3,              # 3 epochs is usually enough for 300+ samples
    weight_decay=0.01,
    save_total_limit=1,
    logging_steps=10,
    fp16=False,                      # FP32 ensures stability (No Loss=0 bug)
    optim="adafactor",               # Saves memory
    gradient_checkpointing=True,
    report_to="none"
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    processing_class=tokenizer,
    data_collator=data_collator
)

trainer.train()

"""Step 6: Save Model"""

save_path = "final_flan_t5_model"
trainer.save_model(save_path)
tokenizer.save_pretrained(save_path)
print(f"Model saved to {save_path}")

"""Step 7: Generate Summaries for target videos"""

from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import pandas as pd
from datasets import load_dataset
from tqdm import tqdm
import torch
import gc
import re

# 1. Define the specific Target IDs
target_ids = [
    "ToZyCA0uvoI","6_q9DbX35kk","udS2OPohs_s","f1y9wDDxWnA","j-3vuBynnOE",
    "wlLydjaHSWc","8_UBDTHAHqY","-0ce136MdsA","C7mUYocWdG0","hAooAOFRsYc",
    "BTLCdge7uSQ","FC-R4MlIqrc","gxMOoNQEIjE","g_yMowQikOE","fEKZC9mta8w",
    "hkw-WDBipgo","U3zmekzQ8WQ","5IRlUVrEVL8","T35ba_VXkMY","o75ybZ-6Uu8",
    "275_oeBw3vY","Nq3auVtvd9Q","ixathu7U-LQ","_8KNb5iqblE","iRrySp4VoPs",
    "v2GRWzIhaqQ","rHQPBqMULXo","lYDiSCDcxmc","qmPfQOU7aHU","_1CaUOHhI6U"
]

# 2. Force Load & Filter Data (Bypassing previous functions)
print("Loading dataset from Hugging Face...")
dataset = load_dataset('jamescalam/youtube-transcriptions', split='train')
full_df = pd.DataFrame(dataset)

# Group by video_id to combine text (Just in case the raw data is chunked)
print("Processing transcripts...")
# Combine sentences into full transcript
full_df['text'] = full_df['text'].astype(str)
grouped_df = full_df.groupby(['video_id', 'title', 'url'])['text'].apply(' '.join).reset_index()
grouped_df.rename(columns={'text': 'full_transcript'}, inplace=True)

# FILTER: Keep ONLY the target IDs
test_df = grouped_df[grouped_df['video_id'].isin(target_ids)].copy()

# Sort to match the order of target_ids (Optional, but good for checking)
test_df['video_id'] = pd.Categorical(test_df['video_id'], categories=target_ids, ordered=True)
test_df = test_df.sort_values('video_id')

print(f"Found {len(test_df)} out of {len(target_ids)} target videos.")

if len(test_df) == 0:
    raise ValueError("No matching videos found! Please check the IDs.")

# 3. Load Model
print("Loading Fine-Tuned Model...")
save_path = "final_flan_t5_model"
tokenizer = AutoTokenizer.from_pretrained(save_path)
model = AutoModelForSeq2SeqLM.from_pretrained(save_path, device_map="auto")

# 4. Helper Functions (Cleaning & Post-processing)
def clean_input_text(text):
    # Skip intro (first 600 chars)
    start_index = 600
    if len(text) < 1000: return text
    return text[start_index : start_index + 3500]

def post_process_summary(text):
    # Remove "summary:" prefixes
    pattern = r'^(summary|summarize|summarise|content)[:\s-]*'
    text = re.sub(pattern, '', text, flags=re.IGNORECASE).strip()
    # Capitalize first letter
    if text: text = text[0].upper() + text[1:]
    # Ensure ends with punctuation
    last_punct = max(text.rfind('.'), text.rfind('!'), text.rfind('?'))
    if last_punct != -1: text = text[:last_punct+1]
    return text

# 5. Generate Loop
results = []
print("Generating Summaries for Targets...")

for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):
    transcript = row['full_transcript']

    # Pre-process
    core_content = clean_input_text(transcript)
    input_text = "summarize: " + core_content

    try:
        inputs = tokenizer(input_text, return_tensors="pt", max_length=1024, truncation=True).to("cuda")

        outputs = model.generate(
            **inputs,
            max_length=150,
            min_length=40,
            num_beams=4,
            no_repeat_ngram_size=3,
            repetition_penalty=1.2,
            early_stopping=True
        )

        raw_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
        final_summary = post_process_summary(raw_summary)

    except Exception as e:
        final_summary = f"Error: {e}"

    results.append({
        "video_id": row['video_id'],
        "title": row['title'],
        "fine_tuned_summary": final_summary
    })

    del inputs
    del outputs
    torch.cuda.empty_cache()

# 6. Save
filename = "week4_final_predictions.csv"
final_df = pd.DataFrame(results)
final_df.to_csv(filename, index=False)

print("\n=== Generation Complete ===")
print(final_df[['video_id', 'title', 'fine_tuned_summary']].head())

# Download block
from google.colab import files
import os
if os.path.exists(filename):
    files.download(filename)

# # Export gold_dataset.csv to local machine
# from google.colab import files
# import os

# filename = 'week4_final_predictions.csv'

# if os.path.exists(filename):
#     print(f"Found {filename}, preparing download...")
#     files.download(filename)
#     print("Download initiated. Check your browser downloads.")
# else:
#     print(f"Error: {filename} not found. Make sure generation ran successfully.")

"""Step 8: Automatic Evaluation (ROUGE Score)

To quantitatively assess our model's performance, we utilized the ROUGE score (Recall-Oriented Understudy for Gisting Evaluation), which measures the overlap between the model-generated summaries (Candidate) and the high-quality Gemini-generated summaries (Reference). Specifically, we evaluated ROUGE-1 for content coverage (unigrams), ROUGE-2 for fluency (bigrams), and ROUGE-L for sentence structure similarity. Since we lack pre-generated "ground truth" summaries for the 30 specific target videos, we instead conducted this evaluation on a random sample of 50 entries from our curated Gold Dataset, where verified reference summaries already exist, allowing for a rigorous and scientific measurement of the model's summarization quality.
"""

# Install standard evaluation libraries
!pip install rouge_score evaluate

import evaluate
from tqdm import tqdm
import random

# 1. Load Data (Ground Truth)
filename = "gold_dataset_merged_final.csv"

# Check if file exists
import os
if os.path.exists(filename):
    df = pd.read_csv(filename)
    df = df.dropna(subset=['target_summary', 'full_transcript'])

    # We take a random sample of 50 items to evaluate performance
    # (Evaluating all 360 takes too long for a quick check)
    if len(df) > 50:
        eval_df = df.sample(50, random_state=42) # random_state ensures reproducibility
    else:
        eval_df = df

    print(f"Evaluating on {len(eval_df)} samples...")
else:
    raise ValueError(f"{filename} not found. Please upload your training data.")

# 2. Load Metric & Model
rouge = evaluate.load("rouge")

save_path = "final_flan_t5_model"
tokenizer = AutoTokenizer.from_pretrained(save_path)
model = AutoModelForSeq2SeqLM.from_pretrained(save_path, device_map="auto")

# 3. Helper for Inference (Same logic as before)
def clean_input(text):
    start_index = 600
    if len(text) < 1000: return text
    return text[start_index : start_index + 3500]

# 4. Evaluation Loop
candidates = [] # What your T5 model generated
references = [] # What Gemini generated (The Gold Standard)

print("Running Inference for Evaluation...")

for index, row in tqdm(eval_df.iterrows(), total=len(eval_df)):
    transcript = row['full_transcript']
    gold_summary = row['target_summary']

    # Preprocess
    core_content = clean_input(transcript)
    input_text = "summarize: " + core_content

    try:
        inputs = tokenizer(input_text, return_tensors="pt", max_length=1024, truncation=True).to("cuda")

        outputs = model.generate(
            **inputs,
            max_length=150,
            min_length=30,
            num_beams=4,
            no_repeat_ngram_size=3,
            early_stopping=True
        )

        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)

        candidates.append(prediction)
        references.append(gold_summary)

    except Exception as e:
        print(f"Error at index {index}: {e}")

# 5. Compute Scores
print("\nComputing ROUGE scores...")
results = rouge.compute(predictions=candidates, references=references, use_stemmer=True)

# 6. Display Results
print("\n=== Automatic Evaluation Results ===")
print(f"ROUGE-1 (Unigram Overlap): {results['rouge1']:.4f}")
print(f"ROUGE-2 (Bigram Overlap):  {results['rouge2']:.4f}")
print(f"ROUGE-L (Longest Sequence):{results['rougeL']:.4f}")
print(f"ROUGE-Lsum (Summary Level):{results['rougeLsum']:.4f}")

"""The ROUGE scores (R-1: 0.20, R-2: 0.04) indicate that while the model attempts abstractive summarization, the lexical overlap with the reference is low, likely due to the strict input truncation used during inference compared to the full context available to the teacher model. To improve this, we could align the training and inference inputs more strictly or increase the dataset size to help the model better converge on the teacher's vocabulary.

Although we fine-tuned the model using a specific dataset of ~350 videos, the model learns the underlying abstractive summarization task, not just the specific content. Therefore, it creates a generalized mapping function $f(Video) \rightarrow Summary$. This allows the system to process any new video ID or URL outside the training set, as long as the input contains a transcript, demonstrating true generalization capability rather than simple memorization.

## Week 5: Data Visuals
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from rouge_score import rouge_scorer

def count_words(text):
    if isinstance(text, str):
        return len(text.split())
    return 0

# Initialize lists
transcript_lengths = []
rouge1_scores = []

# Initialize scorer
scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)

# Calculate lengths and scores
# Assuming eval_df, candidates, and references are available from previous step
for idx, row in eval_df.iterrows():
    # Get length
    t_len = count_words(row['full_transcript'])
    transcript_lengths.append(t_len)

    # Get score for this specific pair
    # Using the corresponding index in the lists
    # Note: Ensure eval_df index aligns with candidates list order
    # If eval_df was shuffled, rely on the lists order if they match row iteration
    # ideally zip them
    pass

# Re-run loop safely using zip to ensure alignment
transcript_lengths = []
rouge1_scores = []

# We also need the matching source transcripts
# Let us use the eval_df transcripts assuming they are in the same order
source_transcripts = eval_df['full_transcript'].tolist()

for src, pred, ref in zip(source_transcripts, candidates, references):
    # Length
    transcript_lengths.append(count_words(src))

    # Score
    scores = scorer.score(ref, pred)
    rouge1_scores.append(scores['rouge1'].fmeasure)

# Create DataFrame for plotting
plot_data = pd.DataFrame({
    'Transcript Length': transcript_lengths,
    'ROUGE-1 Score': rouge1_scores
})

# Create the plot
plt.figure(figsize=(10, 6))
sns.regplot(
    data=plot_data,
    x='Transcript Length',
    y='ROUGE-1 Score',
    scatter_kws={'alpha': 0.5},
    line_kws={'color': 'red'}
)

plt.title('ROUGE-1 Score vs Transcript Length')
plt.xlabel('Transcript Length (words)')
plt.ylabel('ROUGE-1 F1 Score')
plt.grid(True, linestyle='--', alpha=0.7)

# Save the plot
plt.savefig('visual_rouge_vs_length.png', dpi=300, bbox_inches='tight')
plt.show()

from sentence_transformers import SentenceTransformer, util

# 1. Load the model (Re-using the one from Week 1/2)
# Using a small, fast model for evaluation metric
sim_model = SentenceTransformer("all-MiniLM-L6-v2")

# 2. Compute Embeddings
# candidates = model's predictions (list of strings)
# references = gold summaries (list of strings)
print("Encoding candidates...")
cand_embeddings = sim_model.encode(candidates, convert_to_tensor=True)
print("Encoding references...")
ref_embeddings = sim_model.encode(references, convert_to_tensor=True)

# 3. Calculate Cosine Similarity per pair
cosine_scores = []
for i in range(len(candidates)):
    # Compute similarity between candidate i and reference i
    score = util.cos_sim(cand_embeddings[i], ref_embeddings[i]).item()
    cosine_scores.append(score)

# 4. Prepare Data for Plotting
# We compare the ROUGE-1 scores (Lexical) we calculated earlier vs Cosine Scores (Semantic)
# Assuming 'rouge1_scores' list exists from previous visual step
import pandas as pd
df_comparison = pd.DataFrame({
    'Metric Type': ['ROUGE-1 (Lexical)'] * len(rouge1_scores) + ['Cosine Similarity (Semantic)'] * len(cosine_scores),
    'Score': rouge1_scores + cosine_scores
})

# 5. Plot: Violin Plot or Box Plot
plt.figure(figsize=(10, 6))
sns.violinplot(x='Metric Type', y='Score', data=df_comparison, palette="muted", inner="quartile")

plt.title('Lexical Overlap (ROUGE) vs. Semantic Similarity (Embedding)')
plt.ylabel('Score (0.0 - 1.0)')
plt.grid(True, axis='y', linestyle='--', alpha=0.5)

# Add a text annotation explaining the gap
avg_rouge = np.mean(rouge1_scores)
avg_sim = np.mean(cosine_scores)
plt.text(0.5, 0.9, f"Avg Gap: +{avg_sim - avg_rouge:.2f}",
         transform=plt.gca().transAxes, ha='center', fontsize=12,
         bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'))

plt.savefig('visual_semantic_vs_lexical.png', dpi=300, bbox_inches='tight')
plt.show()
