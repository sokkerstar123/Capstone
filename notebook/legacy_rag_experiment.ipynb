{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC1RzR7+0lVi9R+hldjw6H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sokkerstar123/Capstone/blob/main/notebook/legacy_rag_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase I: Retrieval-Augmented Generation (RAG) Prototype\n",
        "\n",
        "**Note:** This notebook documents the initial experimental approach using RAG. While this method was eventually superseded by Fine-Tuning (Knowledge Distillation) in the final product, the logic below demonstrates the vector retrieval architecture designed during Week 2.\n",
        "\n",
        "**Dependencies:**\n",
        "This notebook requires `faiss-cpu`, `sentence-transformers`, and `transformers`."
      ],
      "metadata": {
        "id": "qsvh25aZa8JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install legacy dependencies if running independently\n",
        "!pip install -q faiss-cpu sentence-transformers transformers pandas nltk datasets\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import nltk\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Setup models (Simulated setup for RAG)\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "# Placeholder for tokenizer/generator (using baseline T5 or BART as per original experiment)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "generator = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "RY45a5ksbEUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=100, overlap=20):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = ' '.join(words[i:i+chunk_size])\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Mock retrieval function for demonstration context\n",
        "def retrieve_context(query_text, top_k=3):\n",
        "    # Encode query\n",
        "    query_emb = model.encode([query_text])\n",
        "    # Search for top_k similar chunks\n",
        "    D, I = index.search(np.array(query_emb), top_k)\n",
        "    # Return the retrieved chunk texts\n",
        "    return [all_chunks[i] for i in I[0]]\n",
        "\n",
        "def get_first_n_sentences(text, n):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    return ' '.join(sentences[:n])\n",
        "\n",
        "def construct_rag_prompt(main_text, retrieved_chunks, max_sentences=3):\n",
        "    context_section = \"\\n\".join(retrieved_chunks)\n",
        "    prompt = f\"\"\"You are an expert summarizer. Given the following transcript and relevant context, generate a concise summary.\n",
        "    Transcript:\n",
        "    {main_text}\n",
        "    Retrieved context:\n",
        "    {context_section}\n",
        "    Limit your summary to {max_sentences} sentences.\n",
        "    \"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Core Function\n",
        "def getRagSummary(transcript, exclude_video_id, n=11, top_k=5, max_sentences=2):\n",
        "\n",
        "    main_text = get_first_n_sentences(transcript, n=n)\n",
        "    retrieved = retrieve_context(main_text, top_k=top_k, exclude_video_id=exclude_video_id)\n",
        "    prompt = construct_rag_prompt(main_text, retrieved, max_sentences=max_sentences)\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    # Generate output\n",
        "    # Note: Using the model.generate method directly as per original design\n",
        "    outputs = generator.generate(**inputs, max_length=150)\n",
        "    # Decode the result\n",
        "    rag_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return rag_summary"
      ],
      "metadata": {
        "id": "Jzf4Lp8fbGbb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}